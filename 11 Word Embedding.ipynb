{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72205afd-7051-40c5-b3a2-9719ce4279f5",
   "metadata": {},
   "source": [
    "#### 1. Word Embedding\n",
    "\n",
    "Word2Vec (SkipGram, Continuous Bag of Words – CBOW)  \n",
    "fastText  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3393f8-a595-488c-ba9d-714841cf3483",
   "metadata": {},
   "source": [
    "### What is Word Embedding?\n",
    "Embeddings are dense, continuous vector representations of data, such as words, sentences, or images, in a lower-dimensional space.  \n",
    "They capture the semantic relationships and patterns in the data, where similar items are placed closer together in the vector space.  \n",
    "In machine learning, embeddings are used to convert complex data into numerical form that models can process more easily.  \n",
    "<br>\n",
    "For example, word embeddings represent words based on their meanings and contexts, allowing models to understand relationships like synonyms or analogies.   \n",
    "Embeddings are widely used in tasks like natural language processing, recommendation systems, and image recognition to improve model performance and efficiency.  \n",
    "\n",
    "**Word2Vec** is a widely used method in natural language processing (NLP) that allows words to be represented as vectors in a continuous vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04dc09-c132-45dc-8efc-c4be6c211655",
   "metadata": {},
   "source": [
    "####  Applications of Word Embedding:\n",
    "\n",
    "**Text classification:** Using word embeddings to increase the precision of tasks such as topic categorization and sentiment analysis.  \n",
    "**Named Entity Recognition (NER):** Using word embeddings semantic context to improve the identification of entities (such as names and locations).  \n",
    "**Information Retrieval:** To provide more precise search results, embeddings are used to index and retrieve documents based on semantic similarity.  \n",
    "**Machine Translation:** The process of comprehending and translating the semantic relationships between words in various languages by using word embeddings.  \n",
    "**Question Answering:** Increasing response accuracy and understanding of semantic context in Q&A systems.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ed301-eb06-4010-82e3-c7e2ab2b26cb",
   "metadata": {},
   "source": [
    "### Word embedding using Word2Vec (CBOW and skip gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4a903a36-1989-468b-a760-b623a471f24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.9822592\n",
      "Cosine similarity between 'alice' and 'machines' - CBOW :  0.8997578\n",
      "Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.8719529\n",
      "Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.8462893\n"
     ]
    }
   ],
   "source": [
    "# Python program to generate word vectors using Word2Vec\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Reads ‘alice.txt’ file\n",
    "sample = open(\"text.txt\",encoding=\"utf-8\")\n",
    "s = sample.read()\n",
    "\n",
    "# Replaces escape character with space\n",
    "f = s.replace(\"\\n\", \" \")\n",
    "\n",
    "data = []\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(f):\n",
    "\ttemp = []\n",
    "\n",
    "\t# tokenize the sentence into words\n",
    "\tfor j in word_tokenize(i):\n",
    "\t\ttemp.append(j.lower())\n",
    "\n",
    "\tdata.append(temp)\n",
    "\n",
    "# sg=0: This sets the model to use CBOW (Continuous Bag of Words).  \n",
    "# sg=1: This sets the model to use Skip-gram  \n",
    "# Create CBOW model\n",
    "\n",
    "model1 = gensim.models.Word2Vec(data, min_count=1, vector_size=100, window=5 , sg=0)\n",
    "\n",
    "# Print results\n",
    "print(\"Cosine similarity between 'alice' \" + \"and 'wonderland' - CBOW : \", model1.wv.similarity('alice', 'wonderland'))\n",
    "\n",
    "print(\"Cosine similarity between 'alice' \" + \"and 'machines' - CBOW : \", model1.wv.similarity('alice', 'machines'))\n",
    "\n",
    "# Create Skip Gram model\n",
    "model2 = gensim.models.Word2Vec(data, min_count=1, vector_size=100, window=5, sg=1)\n",
    "\n",
    "# Print results\n",
    "print(\"Cosine similarity between 'alice' \" + \"and 'wonderland' - Skip Gram : \", model2.wv.similarity('alice', 'wonderland'))\n",
    "\n",
    "print(\"Cosine similarity between 'alice' \" + \"and 'machines' - Skip Gram : \", model2.wv.similarity('alice', 'machines'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d7a3d-2feb-4bfc-b920-c83d89901b3d",
   "metadata": {},
   "source": [
    "### Word Embeddings Using FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bb51fef7-bce7-4fcd-9e81-f0b09c500d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'computer': [('user', 0.1565941423177719), ('response', 0.12383823841810226), ('eps', 0.0307049248367548), ('system', 0.025573883205652237), ('interface', 0.005858757067471743), ('survey', -0.03156975656747818), ('minors', -0.054556481540203094), ('human', -0.0668589174747467), ('time', -0.06855934858322144), ('trees', -0.10636082291603088)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "# Example corpus (replace with your own corpus)\n",
    "corpus = common_texts\n",
    "\n",
    "# Training FastText model\n",
    "model = FastText(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "# Example usage: getting embeddings for a word\n",
    "word_embedding = model.wv['computer']\n",
    "\n",
    "# Most similar words to a given word\n",
    "similar_words = model.wv.most_similar('computer')\n",
    "\n",
    "print(\"Most similar words to 'computer':\", similar_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
