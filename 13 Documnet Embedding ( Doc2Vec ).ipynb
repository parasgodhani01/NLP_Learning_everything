{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b8ffd6-b67b-4ee2-b72a-10b5907c2509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 : This is the first document\n",
      "Vector: [-0.01408949 -0.00927592  0.01356978  0.02276812  0.01714495  0.02072828\n",
      "  0.00220343 -0.00846755 -0.02017023 -0.02284916  0.01330294 -0.0234349\n",
      "  0.01391681 -0.0068689  -0.02496642 -0.01623369  0.01198027 -0.0213988\n",
      " -0.00312641  0.00877708]\n",
      "\n",
      "Document 2 : This is the second document\n",
      "Vector: [-0.01347159  0.01339386 -0.02008567 -0.01880541  0.01330884 -0.00035328\n",
      " -0.00184991 -0.01597819  0.02300438 -0.00519029  0.02331674 -0.01419732\n",
      " -0.00645432 -0.01758669  0.00892615  0.02105497  0.01768405  0.00854936\n",
      "  0.00709894  0.00074009]\n",
      "\n",
      "Document 3 : This is the third document\n",
      "Vector: [-0.00977312  0.00224549  0.0145345  -0.01814547  0.00466449 -0.0042301\n",
      "  0.00694907 -0.02081988  0.00267655 -0.00686297 -0.01387998  0.01671938\n",
      "  0.01446606 -0.00574885  0.02399231 -0.00341508 -0.01779622 -0.00494297\n",
      " -0.00300485 -0.00225506]\n",
      "\n",
      "Document 4 : This is the fourth document\n",
      "Vector: [ 0.00382153  0.01764611 -0.01165693 -0.00982382 -0.01263847 -0.01061153\n",
      "  0.01353919  0.00624238  0.01390877  0.01151406  0.01780999  0.00298635\n",
      " -0.01003985 -0.02244707 -0.01930642 -0.0060442  -0.02464624  0.00674406\n",
      "  0.00667505  0.0149291 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# define a list of documents.\n",
    "data = [\"This is the first document\",\n",
    "\t\t\"This is the second document\",\n",
    "\t\t\"This is the third document\",\n",
    "\t\t\"This is the fourth document\"]\n",
    "\n",
    "# preproces the documents, and create TaggedDocuments\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i,doc in enumerate(data)]\n",
    "\n",
    "# train the Doc2vec model\n",
    "model = Doc2Vec(vector_size=20, min_count=2, epochs=50)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data,\n",
    "\t\t\ttotal_examples=model.corpus_count,\n",
    "\t\t\tepochs=model.epochs)\n",
    "\n",
    "# get the document vectors\n",
    "document_vectors = [model.infer_vector(word_tokenize(doc.lower())) for doc in data]\n",
    "\n",
    "# print the document vectors\n",
    "for i, doc in enumerate(data):\n",
    "\tprint(\"Document\", i+1, \":\", doc)\n",
    "\tprint(\"Vector:\", document_vectors[i])\n",
    "\tprint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
