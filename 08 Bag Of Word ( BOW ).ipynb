{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "233070e0-a96c-4194-bd00-7a6f533bd71e",
   "metadata": {},
   "source": [
    "- **Whenever we apply any algorithm in NLP, it works on numbers. We cannot directly feed our text into that algorithm**\n",
    "- **Bag of Words model is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words.**\n",
    "- **This method gives a good accuracy for text classification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e7e14-6b86-4214-b62b-d21f83b5ba1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb01ead4-33c4-48e8-a6ee-d83912a9412a",
   "metadata": {},
   "source": [
    "#### Example \n",
    "<img src=\"bow1.webp\" width=\"500\" height=\"400\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9011511-762d-4851-831e-6f4c408cb1ca",
   "metadata": {},
   "source": [
    "### Step 1 : Preprocess the data\n",
    "- Convert text to lower case.\n",
    "- Remove all non-word characters.   \n",
    "- Remove all punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f55403-baff-42d4-8024-14075f392875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ae88a16-596d-45aa-9de9-eb0dad9bfcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d315c59-6f01-48d0-96d1-017da39b99a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Beans. I was trying to explain to somebody as we were flying in, that’s corn.  That’s beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen in a long time, and somehow he has not aged and I have. And it’s great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didn’t speak at the commencement.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4afe8ef-bad3-4e9d-953e-6d261ee684db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i]=dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f601235-0971-4e24-8149-0faaf5c397de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    0\n",
      "0                                              beans \n",
      "1   i was trying to explain to somebody as we were...\n",
      "2                                       that s beans \n",
      "3   and they were very impressed at my agricultura...\n",
      "4   please give it up for amaury once again for th...\n",
      "5   i have a bunch of good friends here today incl...\n",
      "6   i also noticed by the way former governor edga...\n",
      "7                 and it s great to see you governor \n",
      "8   i want to thank president killeen and everybod...\n",
      "9   and i am deeply honored at the paul douglas aw...\n",
      "10  he is somebody who set the path for so much ou...\n",
      "11  now i want to start by addressing the elephant...\n",
      "12  i know people are still wondering why i didn t...\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083ff1a0-a3b0-45e1-aa58-6cdf093c600c",
   "metadata": {},
   "source": [
    "### Step #2 : Obtaining most frequent words in our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47d23684-9bbc-469e-a6c7-97e6fdbdf68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bag of word model\n",
    "\n",
    "word2count = {}\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6138afd3-c5df-495e-9d91-e31302c3b51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beans            2\n",
      "i               12\n",
      "was              1\n",
      "trying           1\n",
      "to               8\n",
      "                ..\n",
      "wondering        1\n",
      "why              1\n",
      "didn             1\n",
      "speak            1\n",
      "commencement     1\n",
      "Length: 118, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(word2count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5f808e7a-e27a-4b40-b7f2-66b3e00fcfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq \n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67485dff-e7cf-42e0-b3cb-3bc129d89f52",
   "metadata": {},
   "source": [
    "### Step #3 : Building the Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c5b1bb78-fa41-4678-9246-a3f2b504bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)  # Ensures all elements are printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e3c7c2d2-48ae-4d0e-b57c-be36efb4de5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      "  1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0]\n",
      " [0 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      " [1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for data in dataset:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    X.append(vector)\n",
    "X = np.asarray(X)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b96d53-aecb-46e6-b771-a4c782d5e429",
   "metadata": {},
   "source": [
    "<img src='bow.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aafa5bd-f97c-4346-a594-890d4b3764e1",
   "metadata": {},
   "source": [
    "## Using Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7b130ef7-6469-49d8-a052-525ef07cc988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai' 'field' 'helps' 'improve' 'is' 'learning' 'machine' 'models' 'nlp'\n",
      " 'of']\n",
      "[[1 1 0 0 1 0 0 0 1 1]\n",
      " [0 0 1 1 0 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample texts\n",
    "documents = [\n",
    "    \"NLP is a field of AI.\",\n",
    "    \"Machine learning helps NLP models improve.\"\n",
    "]\n",
    "\n",
    "# Create BoW model\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to array and show feature names\n",
    "print(vectorizer.get_feature_names_out())  \n",
    "print(X.toarray())  # BoW matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870fad0-aaa4-4ef3-b654-73804979ecf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
