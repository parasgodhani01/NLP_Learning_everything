{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a258cc3e-8f54-4ac8-8b26-8127126568a6",
   "metadata": {},
   "source": [
    "### Pre-Trained Embedding\n",
    "\n",
    "ELMo (Embeddings from Language Models)  \n",
    "BERT (Bidirectional Encoder Representations from Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b5c38a-de08-47f5-b646-5c62cccd84ad",
   "metadata": {},
   "source": [
    "### Embeddings from Language Models(ELMo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb04dfa-447e-4fce-babb-8cde3e285ec1",
   "metadata": {},
   "source": [
    "- ELMo is an NLP framework developed by AllenNLP. ELMo word vectors are calculated using a two-layer bidirectional language model (biLM). Each layer comprises forward and backward pass.  \n",
    "- Unlike Glove and Word2Vec, ELMo represents embeddings for a word using the complete sentence containing that word. Therefore, ELMo embeddings are able to capture the context of the word used in the sentence and can generate different embeddings for the same word used in a different context in different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "737aa262-f437-41a3-a160-fccb3920db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02380a0-9cb5-41b3-bf61-11518da500c9",
   "metadata": {},
   "source": [
    "### BERT Model â€“ NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e74456-0ccb-47ba-86e7-e90f8ed1c432",
   "metadata": {},
   "source": [
    "- BERT (Bidirectional Encoder Representations from Transformers) leverages a transformer-based neural network to understand and generate human-like language. BERT employs an encoder-only architecture.  \n",
    "- In the original Transformer architecture, there are both encoder and decoder modules. The decision to use an encoder-only architecture in BERT suggests a primary emphasis on understanding input sequences rather than generating output sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c3bb8-9b5b-4d4b-b4f0-3453a50d8b53",
   "metadata": {},
   "source": [
    "1) Pre-training and Fine-tuning BERT Model\n",
    "    - The BERT model undergoes a two-step process:\n",
    "2) Pre-training on Large amounts of unlabeled text to learn contextual embeddings.\n",
    "    - Fine-tuning on labeled data for specific NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85fcb600-8699-40c2-bcc5-8de1a300c429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [101, 24705, 1204, 17095, 1942, 1110, 170, 1846, 2235, 1872, 1118, 3353, 1592, 2240, 117, 1359, 1113, 1103, 15175, 1942, 113, 9066, 15306, 11689, 118, 3972, 13809, 23763, 114, 4220, 119, 102]\n",
      "Tokens: ['[CLS]', 'Cha', '##t', '##GP', '##T', 'is', 'a', 'language', 'model', 'developed', 'by', 'Open', '##A', '##I', ',', 'based', 'on', 'the', 'GP', '##T', '(', 'Gene', '##rative', 'Pre', '-', 'trained', 'Trans', '##former', ')', 'architecture', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Input text\n",
    "text = 'ChatGPT is a language model developed by OpenAI, based on the GPT (Generative Pre-trained Transformer) architecture. '\n",
    "\n",
    "# Tokenize and encode the text\n",
    "encoding = tokenizer.encode(text)\n",
    "\n",
    "# Print the token IDs\n",
    "print(\"Token IDs:\", encoding)\n",
    "\n",
    "# Convert token IDs back to tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding)\n",
    "\n",
    "# Print the corresponding tokens\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
