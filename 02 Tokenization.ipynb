{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a07ad931-ab6e-4f54-9890-43d3a38fef77",
   "metadata": {},
   "source": [
    "### What is Tokenization ?\n",
    "- Tokenization is a fundamental process in Natural Language Processing (NLP) that involves breaking down a stream of text into smaller units called tokens.\n",
    "- Tokens are typically words or sub-words in the context of natural language processing.\n",
    "- The tokens within a document can be used as vector, transforming an unstructured text document into a numerical data structure suitable for machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e118ae-1a4e-488c-a117-7dd4f8da9194",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6b24b-a77f-4b82-95cc-3441f9ac03e3",
   "metadata": {},
   "source": [
    "###  Types of Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1ba84-8897-4f73-83b9-75d2eef0f744",
   "metadata": {},
   "source": [
    "**1. Word Tokenization**  \n",
    "- Word tokenization divides the text into individual words. Many NLP tasks use this approach, in which words are treated as the basic units of meaning.\n",
    "\n",
    "**Example:**  \n",
    "\n",
    "**Input** :\"Tokenization is an important NLP task.\"  \n",
    "**Output**: [\"Tokenization\", \"is\", \"an\", \"important\", \"NLP\", \"task\", .\"].\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e2034e-6c50-4d03-81d2-5d91223d35d5",
   "metadata": {},
   "source": [
    "**2. Sentence Tokenization:**  \n",
    "- The text is segmented into sentences during sentence tokenization. This is useful for tasks requiring individual sentence analysis or processing.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**Input:** \"Tokenization is an important NLP task. It helps break down text into smaller units.\"  \n",
    "**Output:** [\"Tokenization is an important NLP task.\", \"It helps break down text into smaller units.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e9dd4-216d-48b8-bddf-3c21827228da",
   "metadata": {},
   "source": [
    "**3. Subword Tokenization:**  \n",
    "Subword tokenization entails breaking down words into smaller units, which can be especially useful when dealing with morphologically rich languages or rare words.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**Input:** \"tokenization\"  \n",
    "**Output:** [\"token\", \"ization\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8d6060-cade-4b37-9a75-027269b71156",
   "metadata": {},
   "source": [
    "**4. Character Tokenization:**   \n",
    "This process divides the text into individual characters. This can be useful for modelling character-level language.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**Input:** \"Tokenization\"  \n",
    "**Output:** [\"T\", \"o\", \"k\", \"e\", \"n\", \"i\", \"z\", \"a\", \"t\", \"i\", \"o\", \"n\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae61d5c-62e9-4f60-9f12-be533cb7f9bc",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bda58e-c80f-4597-8409-20d84beb2290",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1b41afd4-c6ff-4b01-b15c-8ac203816075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.',\n",
       " 'Welcome to GeeksforGeeks.',\n",
       " 'You are studying NLP article.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article.\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e49c3ca4-9dee-4cd1-80be-fe4ff02ef384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'everyone', '.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "text = \"Hello everyone. Welcome to GeeksforGeeks.\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd92ed-d0fc-4c43-88ec-8805c603c302",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd4f65-bf2d-4013-b6fe-303cc41f0c94",
   "metadata": {},
   "source": [
    "### Limitations of Tokenization\n",
    "- Tokenization is unable to capture the meaning of the sentence hence, results in **ambiguity.**  \n",
    "- In certain languages like Chinese, Japanese, Arabic, lack distinct spaces between words. Hence, there is an **absence of clear boundaries** that complicates the process of tokenization.  \n",
    "- Text may also include more than one word, for example email address, URLs and **special symbols**, hence it is difficult to decide how to tokenize such elements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
